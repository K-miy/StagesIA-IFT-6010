{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-préparation : Évaluation de la qualité des données\n",
    "---\n",
    "## Collecte et colligation : Que regarder ?\n",
    "### VALIDITE\n",
    "- Existe-t-il une relation directe entre l'activité et ce qui est mesuré?\n",
    "- Les données sont-elles désagrégées de manière appropriée?\n",
    "- Les personnes qui collectent les données sont-elles qualifiées et bien supervisées?\n",
    "- Des mesures sont-elles prises pour corriger les erreurs de données connues?\n",
    "- Les problèmes de collecte de données connus ont-ils été correctement évalués?\n",
    "- Des mesures sont-elles prises pour limiter les erreurs de transcription?\n",
    "- Les problèmes de qualité des données sont-ils clairement décrits dans les rapports finaux?\n",
    "\n",
    "### FIABILITÉ\n",
    "- Un processus de collecte de données cohérent est-il utilisé d'année en année, d'une source à l'autre?\n",
    "- Existe-t-il des procédures permettant un examen périodique de la collecte, de la maintenance et du suivi des données, et documentées par écrit?\n",
    "\n",
    "### TEMPORALITÉ\n",
    "- Un calendrier de collecte de données régularisé est-il en place pour répondre aux besoins de gestion du programme?\n",
    "- Les données sont-elles correctement stockées et facilement disponibles?\n",
    "\n",
    "### PRECISION\n",
    "- Existe-t-il une méthode pour détecter les données en double?\n",
    "- Existe-t-il une méthode pour détecter les données manquantes?\n",
    "\n",
    "### INTÉGRITÉ\n",
    "- Des mesures de protection appropriées sont-elles en place pour empêcher les modifications non autorisées des données?\n",
    "- Un examen indépendant des résultats rapportés est-il nécessaire?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ce que vous cherchez a-t-il un sens?\n",
    "\n",
    "Regardez les données. S'il s'agit d'un jeu de données volumineux, examinez les 20 premières lignes, les 20 dernières lignes et un échantillon aléatoire de 20 lignes. Vous trouverez ci-dessous des questions que vous devriez vous poser, indiquées par les puces..\n",
    "\n",
    "### Est-ce que ce que je cherche à donner un sens? Les données correspondent-elles à l'étiquette de la colonne?\n",
    "Y a-t-il des noms dans les colonnes des noms, des adresses dans les colonnes des adresses, des numéros de téléphone dans la colonne des numéros de téléphone? Ou y a-t-il des données différentes dans les colonnes?\n",
    "\n",
    "### Les données respectent-elles les règles appropriées pour son champ?\n",
    "Les caractères d'un nom sont-ils uniquement alphabétiques (Brendan) ou contiennent-ils des chiffres (B4rendan)?\n",
    "La partie numérique d'un numéro de téléphone est-elle de 10 chiffres (5558675309) ou non (675309)?\n",
    "\n",
    "### Calculer des statistiques récapitulatives pour les données numériques. Ont-elles un sens?\n",
    "Si vous traitez avec des données de temps écoulé, la valeur minimale est-elle négative (- 10 secondes)?\n",
    "Si vous traitez avec des données de salaire annuel pour les travailleurs en col bleu, la valeur maximale est-elle quelque chose de bizarre (1 000 000 $)?\n",
    "\n",
    "### Combien de valeurs sont nulles?\n",
    "Le nombre de NULL est-il acceptable? Existe-t-il un modèle indiquant où il y a des valeurs nulles?\n",
    "\n",
    "### Y a-t-il des doublons et sont ils acceptables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les erreurs communes à éviter\n",
    "\n",
    "**1. Préparer les données pour l'analyse sans objectif clair.**\n",
    "L'accès à toutes les données du monde est essentiellement inutile sans objectif direct. Certes, il peut être intéressant de consulter des tas de données volumineuses, mais pour des raisons de croissance et d’innovation, des objectifs clairement définis aideront à rationaliser l’analyse. Nous vous recommandons de convenir par écrit de ces objectifs pour vous assurer, ainsi que votre équipe, de rester sur la bonne voie lors de la préparation de vos données pour analyse. \n",
    "\n",
    "**2. Déprioriser la visualisation des données.**\n",
    "Lors de la préparation des données pour l’analyse, il est extrêmement facile de se perdre dans les chiffres sans penser à la présentation finale ni même à la révision de l’analyse des données. La présentation ou la visualisation est importante, car c’est ainsi que vous, votre équipe et les autres visualisez et interprétez les données. \n",
    "\n",
    "**3. Ignorer les problèmes hors du champ des données.**\n",
    "N'oubliez pas que, dans le monde des données, d'autres mesures allant bien au-delà des chiffres et des performances numériques peuvent parfois être prises en compte. Il peut y avoir des problèmes éthiques, culturels ou philosophiques en jeu qui peuvent prévaloir sur l'analyse de données pure. Soyez sensible à ces points de douleur potentiels pour mieux comprendre comment ils peuvent influer sur vos résultats finaux.\n",
    "\n",
    "**4. Entrer les mauvaises données.**\n",
    "Il est même possible que vous commettiez une erreur lors de la saisie des données de base. La saisie ou la fusion d'informations dans la mauvaise ligne ou colonne ou l'ajout d'un zéro accidentel à la fin d'un nombre sont toutes des erreurs humaines incroyablement courantes lors de la préparation des données pour l'analyse.\n",
    "Lorsqu'il s'agit d'analyser des données, tout processus minimisant le risque d'erreur humaine est extrêmement positif. Testez une procédure sur un sous ensemble des données et automatisez la.\n",
    "\n",
    "**5. Analyser une population (trop) petite.**\n",
    "Bien qu’il n’existe bien entendu rien de mal à élaborer des analyses pour une petite population, il est important de savoir que les données risquent de ne pas fournir autant d’informations utiles que si votre population était plus nombreuse. Les populations plus petites tendent à produire plus de personnes aberrantes sans suffisamment de corrélations pour discerner ce qui se passe réellement. \n",
    "\n",
    "**6. Normes de nommage incohérentes ou confuses.**\n",
    "Dès le départ, l'organisation et la cohérence sont essentielles lors de la préparation des données pour l'analyse. Si vos conventions de dénomination sont légèrement différentes, vos données sont potentiellement très problématiques. Assurez-vous de mettre en place un système de convention de nommage simplifié et partagé apr tous avant de plonger dans l'analyse. Utilisez des termes clairs et qui auront du sens pour ceux avec qui vous envisagez de partager votre analyse. \n",
    "\n",
    "**7. Attention à la duplication!**\n",
    "Cela peut sembler une évidence, mais la duplication est une erreur plutôt commune lors de la préparation des données pour l'analyse. Dupliquer même une infime entrée faussera vos données de manière imprécise, entraînant des prévisions corrompues ou une prise de décision médiocre.\n",
    "\n",
    "**8. Analyse des données altérées.**\n",
    "Vous avez besoin de données propres. Le nettoyage des données peut prendre du temps. Nous avons déjà discuté de la nécessité d'éviter les données en double, mais nous suggérons également d'éliminer les données aberrantes, les données incorrectes, les données manquantes ou les données dépourvues de logique.\n",
    "\n",
    "**9. Mauvaise connexion des données.**\n",
    "Si vous travaillez avec un ensemble de données volumineux, des informations vous parviendront probablement de différentes sources. Vous voulez vous assurer que votre pripeline de préparation extrait des données de la ou des sources appropriées et que ces données sont compatibles avec lui. \n",
    "\n",
    "**10. Utilisation de données obsolètes.**\n",
    "L’utilisation de données obsolètes peut se produire en cas de problème d’intégration ou de saisie de votre source de données (voir le numéro 9). Portez une attention particulière aux délais si vous êtes chargé d’examiner des données en temps réel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CDEWTP-vNB_R"
   },
   "source": [
    "<h1 align=center> Préparation des Données </h1>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Dans cette séquence, nous allons voir différentes techniques permettant de préparer adéquatement les données avant de les fournir à un ou plusieurs algorithmes d'apprentissage automatique.\n",
    "\n",
    "Nous aborderons notamment les points suivants :\n",
    "1. Le nettoyage et les aberrations statistiques\n",
    "2. L'imputation de données manquantes\n",
    "3. Équilibrage de données déséquilibrées\n",
    "4. Transformation des caractéristiques\n",
    "    1. *rescaling* et *normalizing* ([0,1] ou [-1,1]), *standardizing* (loi normale)\n",
    "    2. Représentation matricielle de données catégorisées\n",
    "    3. Réduction de la dimensionnalité ou création de caractéristiques\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage et Aberrations\n",
    "### Identifier les données dupliquées\n",
    "\n",
    "**Compter les valeurs uniques d'une colonne:**\n",
    "\n",
    "```\n",
    "df.groupby('ma_categorie').id_user.nunique()\n",
    "```\n",
    "\n",
    "**Show duplicate values for one column:**\n",
    "\n",
    "```\n",
    "pd.concat(i for _, i in df.groupby('id_user') if len(i) > 1)\n",
    "```\n",
    "\n",
    "Dans le cas où vous voudriez montrer les valeurs dupliquées pour une combinaison de colonnes, remplacez `id_user` par la liste des colonnes à considérer (e.g.  ['id_user', 'pays'])\n",
    "\n",
    "### Retirer les données dupliquées\n",
    "\n",
    "```\n",
    "df = df.drop_duplicates(subset='id_user', keep=False)\n",
    "```  \n",
    "\n",
    "### Identifier les données aberrantes\n",
    "\n",
    "Séquence inspirée des exemples de code de la librairie `scikit-learn` de [Alexandre Gramfort](mailto:alexandre.gramfort@inria.fr) et [Albert Thomas](mailto:albert.thomas@telecom-paristech.fr) (License: BSD 3 clause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['contour.negative_linestyle'] = 'solid'\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "\n",
    "# Paramètres des exemples\n",
    "n_samples = 300\n",
    "outliers_fraction = 0.15\n",
    "n_outliers = int(outliers_fraction * n_samples)\n",
    "n_inliers = n_samples - n_outliers\n",
    "\n",
    "# Definition des datasets\n",
    "blobs_params = dict(random_state=0, n_samples=n_inliers, n_features=2)\n",
    "datasets = [\n",
    "    make_blobs(centers=[[0, 0], [0, 0]], cluster_std=0.5,\n",
    "               **blobs_params)[0],\n",
    "    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[0.5, 0.5],\n",
    "               **blobs_params)[0],\n",
    "    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[1.5, .3],\n",
    "               **blobs_params)[0],\n",
    "    4. * (make_moons(n_samples=n_samples, noise=.05, random_state=0)[0] -\n",
    "          np.array([0.5, 0.25])),\n",
    "    14. * (np.random.RandomState(42).rand(n_samples, 2) - 0.5)]\n",
    "\n",
    "# Mesh pour afficher le contour de décision\n",
    "xx, yy = np.meshgrid(np.linspace(-7, 7, 150),\n",
    "                     np.linspace(-7, 7, 150))\n",
    "\n",
    "# fonction d'affichage des datasets avec coloration de l'algorithme et ajout d'outliers\n",
    "def plot_datasets(name='', algorithm=None, addOutliers=True):\n",
    "    # Taille des figures\n",
    "    plt.figure(figsize=(len(datasets) * 2 + 3, 3))\n",
    "    plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                        hspace=.01)\n",
    "    plot_num = 1\n",
    "    rng = np.random.RandomState(42)\n",
    "    \n",
    "    for i_dataset, X in enumerate(datasets):\n",
    "        \n",
    "        y_pred = np.repeat(0,len(X))\n",
    "        # Add outliers\n",
    "        if addOutliers:\n",
    "            X = np.concatenate([X, rng.uniform(low=-6, high=6,size=(n_outliers, 2))], axis=0)\n",
    "            #On met la vérité dans la prédiction si on fait pas de prédiction\n",
    "            y_pred = np.concatenate([y_pred,np.repeat(1,n_outliers)],axis=0)\n",
    "\n",
    "        t0 = 0\n",
    "        t1 = 0\n",
    "        ax = plt.subplot(1,len(datasets), plot_num)\n",
    "        if algorithm != None:\n",
    "            t0 = time.time()\n",
    "            algorithm.fit(X)\n",
    "            t1 = time.time()\n",
    "            if i_dataset == 0:\n",
    "                plt.title(name, size=18)\n",
    "            # Fit les données et tag les outliers\n",
    "            if name == \"Local Outlier Factor\": # LOF n'implémente pas predict\n",
    "                y_pred = algorithm.fit_predict(X)\n",
    "            elif name == \"KDE\": # KDE non plus\n",
    "                # Score samples\n",
    "                pred = np.exp(algorithm.fit(X).score_samples(X))\n",
    "                n = sum(pred < 0.05)\n",
    "                outlier_ind = np.asarray(pred).argsort()[:n]\n",
    "                y_pred = np.array([-1 if i in outlier_ind else 1 for i in range(len(X))])\n",
    "            elif name == \"Tukey\": # Et celui la non plus ... obviously\n",
    "                outlier_ind = algorithm.outliers(X)\n",
    "                y_pred = np.array([-1 if i in outlier_ind else 1 for i in range(len(X))])\n",
    "            else: \n",
    "                y_pred = algorithm.fit(X).predict(X)\n",
    "                # Affichage des points et de la ligne de démarquation\n",
    "                Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "                Z = Z.reshape(xx.shape)\n",
    "                plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='black')\n",
    "            plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'), transform=plt.gca().transAxes, size=15, horizontalalignment='right')\n",
    "\n",
    "        colors = np.array(['#377eb8', '#ff7f00'])\n",
    "        ax.scatter(X[:, 0], X[:, 1], s=10, color=colors[(y_pred + 1) // 2])\n",
    "        plt.xlim(-7, 7)\n",
    "        plt.ylim(-7, 7)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plot_num += 1\n",
    "    plt.show()\n",
    "\n",
    "# Affichage des données sans outliers\n",
    "plot_datasets(addOutliers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affichage avec outliers\n",
    "plot_datasets(addOutliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance robuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "algo =  (\"Robust covariance\", EllipticEnvelope(contamination=outliers_fraction))\n",
    "\n",
    "plot_datasets(*algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM mono-classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "algo = (\"One-Class SVM\", svm.OneClassSVM(nu=outliers_fraction, kernel=\"rbf\", gamma=0.1))\n",
    "\n",
    "plot_datasets(*algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forêt d'Isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "algo = (\"Isolation Forest\", IsolationForest(contamination=outliers_fraction, random_state=42))\n",
    "\n",
    "plot_datasets(*algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Facteur d'aberration local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "algo = (\"Local Outlier Factor\", LocalOutlierFactor(n_neighbors=35, contamination=outliers_fraction))\n",
    "\n",
    "plot_datasets(*algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test de Tukey pour les valeurs extrêmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ecart = 1.5\n",
    "\n",
    "# Defini la fonction qui utilise les deviations interquartile avec les quartiles 1 et 3 comme plancher et plafond.\n",
    "class Tukey:\n",
    "    def fit(self, X):\n",
    "        None\n",
    "    def outliers(self,x):\n",
    "        q1 = np.percentile(x, 25)\n",
    "        q3 = np.percentile(x, 75)\n",
    "        iqr = q3-q1 \n",
    "        floor = q1 - ecart * iqr\n",
    "        ceiling = q3 + ecart * iqr\n",
    "        outlier_indices = np.where((x < floor)|(x > ceiling))[0]\n",
    "        print(len(outlier_indices))\n",
    "        return outlier_indices\n",
    "\n",
    "\n",
    "algo = ('Tukey',Tukey())\n",
    "plot_datasets(*algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation de densité par noyau (KDE)\n",
    "\n",
    "Non-parametrique, peut aussi capturer les distributions bimodales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors.kde import KernelDensity\n",
    "\n",
    "# kernel : [‘gaussian’|’tophat’|’epanechnikov’|’exponential’|’linear’|’cosine’]\n",
    "\n",
    "algo = ('KDE', KernelDensity(bandwidth=0.2,kernel='gaussian'))\n",
    "\n",
    "plot_datasets(*algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation de données manquantes \n",
    "Si vous voulez faire simple : `scikit-learn` offre un [`SimpleImputer`](http://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer) et un [`MissingIndicator`](http://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html#sklearn.impute.MissingIndicator) alors que `pandas` offre la méthode [`fillna()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html).\n",
    "\n",
    "Sinon : Fonctionalités de la librairie [`impypute`](https://pypi.org/project/impyute/):\n",
    "- Outils de diagnostic\n",
    "     - Journaux\n",
    "     - Distribution des valeurs nulles\n",
    "     - Comparaison des imputations\n",
    "     - [Test MCAR de Little [1]](#note1)\n",
    "- Imputation de données transversales\n",
    "     - Imputation aléatoire\n",
    "     - K-voisins les plus proches\n",
    "     - Imputation moyenne\n",
    "     - Imputation par mode\n",
    "     - Imputation médiane\n",
    "     - Imputation multivariée par équations chaînées ([MICE](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/))\n",
    "     - Espérance/Maximisation\n",
    "- Imputation de données chronologiques\n",
    "     - Dernière observation reportée\n",
    "     - Fenêtre mobile\n",
    "     - Moyenne mobile intégrée autorégressive (WIP)\n",
    "     \n",
    "Ou bien : [`fancyimpute`](https://pypi.org/project/fancyimpute/) :\n",
    "* `SimpleFill`: Remplace les entrées manquantes par la moyenne ou la médiane de chaque colonne.\n",
    "* `KNN`: imputations du voisin le plus proche qui pondère les échantillons en utilisant la différence quadratique moyenne sur les entités pour lesquelles deux lignes contiennent des données observées.\n",
    "* `SoftImpute`: complétion de la matrice par seuillage souple itératif des décompositions SVD. Inspiré du package [softImpute](https://web.stanford.edu/~hastie/swData/softImpute/vignette.html) pour R, basé sur [Spectral Regularization Algorithms for Learning Large Incomplete Matrices](http://web.stanford.edu/~hastie/Papers/mazumder10a.pdf) de Mazumder et. Al.\n",
    "* `IterativeSVD`: achèvement de la matrice par décomposition itérative SVD de bas rang. Devrait être similaire à SVDimpute de [Missing value estimation methods for DNA microarrays](http://www.ncbi.nlm.nih.gov/pubmed/11395428) de Troyanskaya et. Al.\n",
    "* `IterativeImputer` (ex MICE): Une stratégie pour imputer les valeurs manquantes en modélisant chaque entité avec des valeurs manquantes en fonction des autres entités de manière alternée.\n",
    "* `MatrixFactorization`: Factorisation directe de la matrice incomplète en« U »et« V »de bas rang, avec une pénalité de faible densité L1 sur les éléments de« U »et une pénalité de L2 sur les éléments de« V ». Résolu par descente progressive.\n",
    "* `NuclearNormMinimization`: Implémentation simple de [Exact Matrix Completion via Convex Optimization](http://statweb.stanford.edu/~candes/papers/MatrixCompletion.pdf) d'Emmanuel Candes et Benjamin Recht utilisant [cvxpy](http://www.cvxpy.org). Trop lent pour les grandes matrices.\n",
    "* `BiScaler`: Estimation itérative de la moyenne des rangées/colonnes et des écarts types pour obtenir une double normalisationmatrice. Pas garanti de converger mais fonctionne bien dans la pratique. Tiré de [Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares](http://arxiv.org/abs/1410.2596)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b57ada49-2e30-473d-9e6e-fa57afd87bc5",
    "_uuid": "c66c9a187266122b4fdc5b50ad650988f9e23603"
   },
   "source": [
    "Essayons de manipuler des données manquantes de la compétition Kaggle de titanic[$^1$](https://www.kaggle.com/c/titanic):\n",
    "\n",
    "### Chargement des données\n",
    "\n",
    " - survival: Survival (0 = No, 1 = Yes)\n",
    " - pclass: Ticket class(1 = 1st, 2 = 2nd, 3 = 3rd)\n",
    " - sex: Sex\n",
    " - Age: Age in years\n",
    " - sibsp: Number of siblings / spouses aboard the Titanic\n",
    " - parch: Number of parents / children aboard the Titanic\n",
    " - ticket: Ticket number\n",
    " - fare:Passenger fare\n",
    " - cabin:Cabin number\n",
    " - embarked:Port of Embarkation(C = Cherbourg, Q = Queenstown, S = Southampton)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f271bfd2-8246-485c-82fc-1f9e6610747c",
    "_execution_state": "idle",
    "_uuid": "6251a7617955afc0bfb5bd3cbb4358340f38c450"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "train = pd.read_csv('titanic_train.csv')\n",
    "test = pd.read_csv('titanic_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création des types de colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f12a304c-6520-452e-8135-0cbfc223082c",
    "_execution_state": "idle",
    "_uuid": "44f43996509540d908f166e8b3beb690031b7e5b"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "def enforceTypesTitanic(df):\n",
    "    try:\n",
    "        df.Survived = df.Survived.astype(\"category\")\n",
    "    except:\n",
    "        pass\n",
    "    df.Pclass = df.Pclass.astype(\"category\", categories=[1, 2, 3], ordered=True)\n",
    "    df.Sex = df.Sex.astype(\"category\")\n",
    "    df.Embarked = df.Embarked.astype(\"category\")\n",
    "    \n",
    "\n",
    "enforceTypesTitanic(train)\n",
    "enforceTypesTitanic(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comptage des valeurs nulles dans le dataset (`train` et `test`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "60a9c965-1eb9-4199-9a1b-e24b0ccc7352",
    "_execution_state": "idle",
    "_uuid": "9f36576fed683bf0429450ef52fec336e3172b36"
   },
   "outputs": [],
   "source": [
    "def naSummary(df):\n",
    "    return df.isnull().sum()\n",
    "\n",
    "naSummary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e5d807cb-4a0c-403d-a215-41ca0cc37be4",
    "_execution_state": "idle",
    "_uuid": "2873910fccabd27b0fb5141ce3f42e6589e613b5"
   },
   "outputs": [],
   "source": [
    "naSummary(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "602b7065-1c54-4a77-8e47-4836e68a0a05",
    "_uuid": "008a9e79ab99b1553bd002d0b745a85fa11f08a2"
   },
   "source": [
    "## Validation de la distribution des données Train vs Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ffd616ea-7280-44f5-acf4-cdc44848113f",
    "_execution_state": "idle",
    "_uuid": "1d0136391d9823a89a253cfbe01dcf5ffe3d6364"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gs\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Calculer la taille de la grille en fonction du nombre de caractéristiques \n",
    "def gridSize(nb_features):\n",
    "    a = len(nb_features)\n",
    "    if a%2 != 0:\n",
    "        a += 1\n",
    "    n = np.floor(np.sqrt(a)).astype(np.int64)\n",
    "    while a%n != 0:\n",
    "        n -= 1\n",
    "    m = (a/n).astype(np.int64)\n",
    "    return m,n\n",
    "\n",
    "# Affichage des deux distributions pour chaque colonne des dataframe 1 et 2\n",
    "def distComparison(df1, df2):\n",
    "    \n",
    "    assert (len(df1.columns) == len(df2.columns))\n",
    "    \n",
    "    m,n = gridSize(df1.columns)\n",
    "    coords = list(itertools.product(list(range(m)), list(range(n))))\n",
    "    \n",
    "    # Choix des graphiques pour chaque type de colonne\n",
    "    numerics = df1.select_dtypes(include=[np.number]).columns\n",
    "    cats = df1.select_dtypes(include=['category']).columns\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    axes = gs.GridSpec(m, n)\n",
    "    axes.update(wspace=0.25, hspace=0.25)\n",
    "    # Graphiques pour données numériques : on fait un KDE de la distribution\n",
    "    for i in range(len(numerics)):\n",
    "        x, y = coords[i]\n",
    "        ax = plt.subplot(axes[x, y])\n",
    "        col = numerics[i]\n",
    "        sns.kdeplot(df1[col].dropna(), ax=ax, label='df1').set(xlabel=col)\n",
    "        sns.kdeplot(df2[col].dropna(), ax=ax, label='df2')\n",
    "        \n",
    "    # Graphique pour les données catégoriques : diagramme en baton\n",
    "    for i in range(0, len(cats)):\n",
    "        x, y = coords[len(numerics)+i]\n",
    "        ax = plt.subplot(axes[x, y])\n",
    "        col = cats[i]\n",
    "\n",
    "        df1_temp = df1[col].value_counts()\n",
    "        df2_temp = df2[col].value_counts()\n",
    "        df1_temp = pd.DataFrame({col: df1_temp.index, 'value': df1_temp/len(df1), 'Set': np.repeat('df1', len(df1_temp))})\n",
    "        df2_temp = pd.DataFrame({col: df2_temp.index, 'value': df2_temp/len(df2), 'Set': np.repeat('df2', len(df2_temp))})\n",
    "\n",
    "        sns.barplot(x=col, y='value', hue='Set', data=pd.concat([df1_temp, df2_temp]), ax=ax).set(ylabel='Percentage')\n",
    "\n",
    "# Affichage de la comparaison entre train (sans la colonne des étiquettes) et le test\n",
    "distComparison(train.drop('Survived', 1), test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eb108455-8d35-42b5-a6e6-0d644d30959e",
    "_uuid": "de2217f045c02397cf908b43a38ee9dc9b86fc6b"
   },
   "source": [
    "On se crée une baseline en complétant les données arbitrairement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2412eab3-ac8c-421c-b78d-44b12525f6e4",
    "_execution_state": "idle",
    "_uuid": "c0752bdebb029986336696601ebed13ff61ed521"
   },
   "outputs": [],
   "source": [
    "# Embarked est le port d'embaquement, on complète les deux données manquantes par le premier port.\n",
    "train.Embarked = train.Embarked.fillna('C')\n",
    "\n",
    "# On remplace la donnée Cabin par juste l'infomration sur la connaissance de celle-ci\n",
    "train['CabinKnown'] = pd.Categorical((train.Cabin.isnull() == False))\n",
    "test['CabinKnown'] = pd.Categorical((test.Cabin.isnull() == False))\n",
    "train = train.drop('Cabin', 1)\n",
    "test = test.drop('Cabin', 1)\n",
    "\n",
    "# Et il nous manque une donnée de prix payé dans le test. On choisis une valeur arbitraire.\n",
    "test.Fare = test.Fare.fillna(8.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "681aee3b-eb7f-484d-a19e-60ebf4bbb90e",
    "_execution_state": "idle",
    "_uuid": "9a31b0e3899434d8ad905765693eee5f0a866429"
   },
   "outputs": [],
   "source": [
    "naSummary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dc91c968-cf18-4b2e-be7a-ae9ec3655943",
    "_execution_state": "idle",
    "_uuid": "cb38bd0baa748a18599ad5c09c76985f154cabb1"
   },
   "outputs": [],
   "source": [
    "naSummary(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d36d00c2-6fd3-4b5f-9140-3dff423e0b87",
    "_uuid": "f4046a90e27ad0913eca82b47cc76c2d07850ccd"
   },
   "source": [
    "## Les données sont elles MCAR ?\n",
    "\n",
    "MCAR = missing completely at random.\n",
    "\n",
    "Essentiellement, divisons les données en deux ensembles supplémentaires: Données manquantes et données présentes. Puis vérifions que, si la distribution des variables dans chacun de ces ensembles est la même, alors les données manquent complètement au hasard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cee1ba00-736b-4b63-aca0-589d4a80acdd",
    "_execution_state": "idle",
    "_uuid": "e3655f5dba68b4f2a4eb1e9633daee2a74a4bc54"
   },
   "outputs": [],
   "source": [
    "age_present = train.dropna().drop('Age', 1)\n",
    "age_missing = train[train.isnull().any(axis=1)].drop('Age', 1)\n",
    "\n",
    "age_present.Parch = age_present.Parch.astype('category', categories=list(range(8)), ordered=True)\n",
    "age_missing.Parch = age_missing.Parch.astype('category', categories=list(range(8)), ordered=True)\n",
    "\n",
    "age_present.SibSp = age_present.SibSp.astype('category', categories=list(range(9)), ordered=True)\n",
    "age_missing.SibSp = age_missing.SibSp.astype('category', categories=list(range(9)), ordered=True)\n",
    "\n",
    "distComparison(age_present.drop('Survived', 1), age_missing.drop('Survived', 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ec01755d-01b8-45e8-b57a-7f83ff302927",
    "_uuid": "0fee94f930b9a76b642e4182f2e0a073da427c6c"
   },
   "source": [
    "Il semble que nous ne puissions pas vérifier l’hypothèse MCAR. L'explication semble être que nous sommes moins susceptibles de connaître l'âge des personnes décédées. Comme en témoigne la proportion beaucoup plus grande de passagers de la classe inférieure, le pic plus net dans les tarifs plus bas et une légère asymétrie à l’égard des hommes.\n",
    "\n",
    "De manière plus significative encore, il semble que les personnes qui se sont embarquées à Q ont un taux beaucoup plus élevé d’âge manquant.\n",
    "\n",
    "Remarque: Il serait préférable d'utiliser une mesure plus objective de MCAR, comme le test de Little : (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8be28abb-f498-450a-9d25-4771c432ecec",
    "_uuid": "1eb821dba55457db166b63b8bf943bb1f5a30164"
   },
   "source": [
    "## Baseline de prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "64668e8b-0ae7-4c4e-8062-afdaf708c9d4",
    "_uuid": "5079fff2dea16b57d9c220e40b694a4dde489b38"
   },
   "source": [
    "Sans l'utilisation des données `Age`, notre prédicteur sera un classificateur par forêt aléatoire avec les paramètres affichés. Toutes les estimations d'erreur de test sont obtenues par une validation croisée 10 fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9eb9a7fe-afd8-4986-b76b-adf4bc6337cd",
    "_execution_state": "idle",
    "_uuid": "b8cbe9474eb6ea90e1adf91b38baa619b61948d5"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Préparation des données rapides pour enlever les données catégoriques\n",
    "def prepForModel(df):\n",
    "    new_df = df.copy()\n",
    "    # Classe de cabine en entier\n",
    "    new_df.Pclass = new_df.Pclass.astype(\"int\")\n",
    "    # Sexe binaire\n",
    "    new_df.Sex.cat.categories = [0, 1]\n",
    "    new_df.Sex = new_df.Sex.astype(\"int\")\n",
    "    # Port d'embarquement en entier (on pourrait utiliser une dummy ? tryit ;-))\n",
    "    new_df.Embarked.cat.categories = [0, 1, 2]\n",
    "    new_df.Embarked = new_df.Embarked.astype(\"int\")\n",
    "    # Cabine connue ou non : binaire\n",
    "    new_df.CabinKnown.cat.categories = [0, 1]\n",
    "    new_df.CabinKnown = new_df.CabinKnown.astype(\"int\")\n",
    "    return new_df\n",
    "\n",
    "# Même pipeline pour le train et le test.\n",
    "train_cl = prepForModel(train)\n",
    "test_cl = prepForModel(test)\n",
    "\n",
    "# Sélection des colonnes sans l'Age\n",
    "Xcol = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked', 'CabinKnown']\n",
    "Ycol = 'Survived'\n",
    "X = train_cl.loc[:, Xcol]\n",
    "Y = train_cl.loc[:, Ycol]\n",
    "\n",
    "# Mémorisation des datasets avant qu'on fasse d'autres transformations\n",
    "Xbase = X\n",
    "Ybase = Y\n",
    "\n",
    "# Classification par RF\n",
    "rf = RandomForestClassifier(n_estimators=1000,\n",
    "                           max_depth=None,\n",
    "                           min_samples_split=10)\n",
    "\n",
    "baseline_err = cross_val_score(rf, X, Y, cv=10, n_jobs=-1).mean()\n",
    "print(\"[BASELINE] Estimation RF sur Test (n = {}, 10-fold CV): {}\".format(len(X), baseline_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d2a87fb-4317-4fd2-a6db-f5bd62fb3362",
    "_uuid": "bcad8d2354eb04f7139567624d27ac2d9e73ad5a"
   },
   "source": [
    "#### Suppression simple des données manquantes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e33bf298-f2ea-4434-900a-b69bc9962271",
    "_execution_state": "idle",
    "_uuid": "36c1b8c52a4081287d8ec87813b9ce1dab335680"
   },
   "outputs": [],
   "source": [
    "Xdel = train_cl.dropna().loc[:, Xcol + ['Age']]\n",
    "Ydel = train_cl.dropna().loc[:, Ycol]\n",
    "\n",
    "deletion_err = cross_val_score(rf, Xdel, Ydel, cv=10, n_jobs=-1).mean()\n",
    "print(\"[DELETION] Estimation RF sur Test (n = {}, 10-fold CV): {}\".format(len(Xdel), deletion_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a6ac5c2b-364b-476d-8786-dd9f0fd7a9cc",
    "_uuid": "4dc1d8e53ddc961c92a334367201c89cdc21af3b"
   },
   "source": [
    "#### Substitution par la moyenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "004c75ef-2fcc-481f-a9db-f9d3e13d43ad",
    "_execution_state": "idle",
    "_uuid": "e712cf05d964833a58e3e4709ac83060254149b6"
   },
   "outputs": [],
   "source": [
    "train_cl = prepForModel(train)\n",
    "train_cl.Age = train_cl.Age.fillna(train_cl.Age.mean(skipna=True))\n",
    "\n",
    "Xcol = Xcol + ['Age']\n",
    "\n",
    "Xmean = train_cl.loc[:, Xcol]\n",
    "Ymean = train_cl.loc[:, Ycol]\n",
    "\n",
    "mean_err = cross_val_score(rf, Xmean, Ymean, cv=10, n_jobs=-1).mean()\n",
    "print(\"[MEAN] Estimation RF sur Test (n = {}, 10-fold CV): {}\".format(len(Xmean), mean_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a52e28e5-1333-4c96-bda7-10b36c39c137",
    "_uuid": "ec26ebd70984436e825d406d2616830f0f12e9cc"
   },
   "source": [
    "#### Régression déterministe et aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "da1e76cc-e899-4c66-b0e0-7b8f244edb6a",
    "_execution_state": "idle",
    "_uuid": "2a93932a19fbefa1771ec6cb4231ad9d03308731"
   },
   "outputs": [],
   "source": [
    "train_cl = prepForModel(train)\n",
    "train_reg = train_cl.dropna()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import preprocessing\n",
    "\n",
    "Xrcol = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked', 'CabinKnown']\n",
    "Yrcol = 'Age'\n",
    "\n",
    "X_reg = train_reg.loc[:, Xrcol]\n",
    "Y_reg = train_reg.loc[:, Yrcol]\n",
    "\n",
    "age_lm = LinearRegression()\n",
    "age_lm.fit(X_reg, Y_reg)\n",
    "abs_residuals = np.absolute(Y_reg - age_lm.predict(X_reg))\n",
    "\n",
    "nan_inds = train_cl.Age.isnull().nonzero()[0]\n",
    "train_cl2 = train_cl.copy()\n",
    "\n",
    "for i in nan_inds:\n",
    "    train_cl.set_value(i, 'Age', age_lm.predict(train_cl.loc[i, Xrcol].values.reshape(1, -1)))\n",
    "\n",
    "Xreg = train_cl.loc[:, Xcol]\n",
    "Yreg = train_cl.loc[:, Ycol]\n",
    "    \n",
    "reg_err = cross_val_score(rf, Xreg, Yreg, cv=10, n_jobs=-1).mean()\n",
    "print(\"[DETERMINISTIC REGRESSION] Estimation RF sur Test (n = {}, 10-fold CV): {}\".format(len(Xreg), reg_err))\n",
    "\n",
    "for i in nan_inds:\n",
    "    detreg = age_lm.predict(train_cl2.loc[i, Xrcol].values.reshape(1, -1))\n",
    "    randreg = np.random.normal(detreg, np.random.choice(abs_residuals))\n",
    "    train_cl2.set_value(i, 'Age', randreg)\n",
    "    \n",
    "Xrandreg = train_cl2.loc[:, Xcol]\n",
    "Yrandreg = train_cl2.loc[:, Ycol]\n",
    "    \n",
    "randreg_err = cross_val_score(rf, Xrandreg, Yrandreg, cv=10, n_jobs=-1).mean()\n",
    "print(\"[RANDOM REGRESSION] Estimation RF sur Test (n = {}, 10-fold CV): {}\".format(len(Xrandreg), randreg_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "30dc53c5-cb32-4d44-8d23-d2c806a9cd23",
    "_uuid": "408dbaa3bdcefb881222e942070e050e36bd40f3"
   },
   "source": [
    "#### MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0fe7ab1f-4e5c-4d7a-af4c-988166038c09",
    "_execution_state": "idle",
    "_uuid": "7711d1c85dd8a24028b042e4710b97a0ac5be993"
   },
   "outputs": [],
   "source": [
    "#!pip install fancyimpute \n",
    "# En cas de problème avec cvxpy sur windows : http://www.lfd.uci.edu/~gohlke/pythonlibs/\n",
    "from fancyimpute import IterativeImputer # MICE a été renommé au mois de Septembre ...\n",
    "\n",
    "train_cl = prepForModel(train)\n",
    "\n",
    "X = train_cl.loc[:, Xcol]\n",
    "Y = train_cl.loc[:, Ycol]\n",
    "\n",
    "Xmice = IterativeImputer().fit_transform(X.as_matrix())\n",
    "Ymice = Y\n",
    "\n",
    "mice_err = cross_val_score(rf, Xmice, Y, cv=10, n_jobs=-1).mean()\n",
    "print(\"[MICE] Estimation RF sur Test (n = {}, 10-fold CV): {}\".format(len(Xmice), mice_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d37c7551-67bd-4e7f-94d9-25a950021876",
    "_uuid": "5cb98a3c43ea86f8348b529623eb2cc39f3970c3"
   },
   "source": [
    "#### KNN (Données standardisées)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "605caaa2-c1cf-4b51-a21f-7017044df283",
    "_execution_state": "idle",
    "_uuid": "2115df6de2009ec96f1ca23793136d8d6d25ea69"
   },
   "outputs": [],
   "source": [
    "from fancyimpute import KNN\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "train_cl = prepForModel(train)\n",
    "\n",
    "Xcol = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked', 'CabinKnown']\n",
    "X = train_cl.loc[:, Xcol + ['Age']]\n",
    "Y = train_cl.loc[:, Ycol]\n",
    "\n",
    "def standardize(s):\n",
    "    return s.sub(s.min()).div((s.max() - s.min()))\n",
    "\n",
    "Xnorm = X.apply(standardize, axis=0)\n",
    "kvals = np.linspace(1, 100, 20, dtype='int64')\n",
    "\n",
    "knn_errs = []\n",
    "for k in kvals:\n",
    "    knn_err = []\n",
    "    Xknn = KNN(k=k, verbose=False).fit_transform(Xnorm)\n",
    "    knn_err = cross_val_score(rf, Xknn, Y, cv=10, n_jobs=-1).mean()\n",
    "\n",
    "    knn_errs.append(knn_err)\n",
    "    print(\"[KNN] Estimation RF sur Test (n = {}, k = {}, 10-fold CV): {}\".format(len(Xknn), k, np.mean(knn_err)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cebf6c9d-d62d-4379-914f-2767b4989e6a",
    "_execution_state": "idle",
    "_uuid": "096ca7bc22cf73a4f2cc1c5856725f4b46b5afd9"
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "_ = plt.plot(kvals, knn_errs)\n",
    "_ = plt.xlabel('K')\n",
    "_ = plt.ylabel('10-fold CV Error Rate')\n",
    "\n",
    "knn_err = max(knn_errs)\n",
    "k_opt = kvals[knn_errs.index(knn_err)]\n",
    "\n",
    "Xknn = KNN(k=k_opt, verbose=False).fit_transform(Xnorm)\n",
    "Yknn = Y\n",
    "\n",
    "print(\"[BEST KNN] Estimation RF sur Test (n = {}, k = {}, 10-fold CV): {}\".format(len(Xknn), k_opt, np.mean(knn_err)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b6b05aaf-e846-4225-95c0-e66f807e61ea",
    "_uuid": "c02e770675a39fd1b4c4e3f13639b852f5a24d7d"
   },
   "source": [
    "#### Résumé : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "96847d9e-e09f-48f4-a99f-2abd98331ae4",
    "_execution_state": "idle",
    "_uuid": "b93d5f9dc49de031155455631f9130841cd27d91"
   },
   "outputs": [],
   "source": [
    "errs = {'BEST KNN (k = {})'.format(k_opt): knn_err,  \n",
    "        'DETERMINISTIC REGRESSION': reg_err, \n",
    "        'RANDOM REGRESSION': randreg_err,\n",
    "        'MICE': mice_err,\n",
    "        'MEAN': mean_err,\n",
    "        'DELETION': deletion_err, \n",
    "        'BASELINE': baseline_err}\n",
    "\n",
    "err_df = pd.DataFrame.from_dict(errs, orient='index')\n",
    "err_df.index.name = 'Imputation Method'\n",
    "err_df.reset_index(inplace=True)\n",
    "err_df.columns = ['Imputation', ' Estimation sur Test  (10-fold CV)']\n",
    "\n",
    "ax = sns.barplot(x=err_df.columns[1], y=err_df.columns[0], order=list.sort(list(errs.values())), data=err_df)\n",
    "ax.set_xlabel(err_df.columns[1])\n",
    "ax.set_ylabel('')\n",
    "_ = plt.xlim(0.8, 0.835)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données d'entraînement déséquilibrées\n",
    "\n",
    "### Paradoxe de l'Exactitude\n",
    "\n",
    "Le [paradoxe de l'exactitude](https://en.wikipedia.org/wiki/Accuracy_paradox) est le nom de la situation où vos mesures d'exactitude indiquent que vous avez une excellente exactitude (telle que 90%), mais que l'exactitude ne reflète que la distribution de classe majoritaire sous-jacente.\n",
    "\n",
    "C'est un problème très courant, car l'exactitude est souvent la première mesure que utilisée (par `scikit-learn`) pour évaluer les modèles de nos problèmes de classification.\n",
    "\n",
    "### Mettez tout sur le rouge!\n",
    "\n",
    "Que se passe-t-il dans les modèles lorsque l'entrainement est fait sur un jeu de données déséquilibré? \n",
    "\n",
    "Si une exactitude de 90% est obtenue sur des données déséquilibrées (avec 90% des instances de la classe 1), c'est parce que les modèles examinent les données et décident intelligemment que la meilleure chose à faire est de toujours prédire \"Classe 1\" et atteindre une grande exactitude.\n",
    "\n",
    "Cela se voit mieux lorsque un algorithme basé sur des règles simples est utilisé. En regardant la règle dans le modèle final, on constate qu'il est très probable qu'une seule classe est prédite, quelles que soient les données à prédire.\n",
    "\n",
    "Comment régler ce problème ?\n",
    "\n",
    "#### Collecter plus de données\n",
    "\n",
    "Vous pensez peut-être que c'est idiot, mais la collecte de plus de données est presque toujours négligée.\n",
    "\n",
    "Pouvez-vous collecter plus de données? Prenez une seconde et demandez-vous si vous êtes capable de collecter plus de données sur votre problème.\n",
    "\n",
    "Un ensemble de données plus volumineux pourrait exposer une perspective différente et peut-être plus équilibrée des classes.\n",
    "\n",
    "D'autres exemples de classes mineures peuvent être utiles ultérieurement lorsque nous examinons le rééchantillonnage de votre jeu de données.\n",
    "\n",
    "#### Changer de métrique de performance\n",
    "\n",
    "L'exactitude n'est pas la métrique à utiliser lorsque vous travaillez avec un jeu de données déséquilibré. Nous avons vu que c'est trompeur.\n",
    "\n",
    "Certains indicateurs ont été conçus pour vous raconter une histoire plus véridique lorsque vous travaillez avec des classes déséquilibrées.\n",
    "\n",
    "Vous verrez plus en détails au prochain bloc les différentes mesures de performance:\n",
    "- Matrice de confusion: décomposition des prédictions dans un tableau montrant les prédictions correctes (la diagonale) et les types de prédictions incorrectes effectuées (quelles classes de prédictions incorrectes ont été attribuées).\n",
    "- Exactitude: Mesure de l'exactitude d'un classificateur.\n",
    "- Rappel : Une mesure de la complétude d'un classificateur\n",
    "- Score F1 (ou F-score): moyenne pondérée de la précision et du rappel.\n",
    "- Kappa (ou [kappa de Cohen](https://en.wikipedia.org/wiki/Cohen%27s_kappa)): précision de la classification normalisée par le déséquilibre des classes dans les données.\n",
    "- Courbes ROC: À l'instar de la précision et du rappel, l'exactitude est divisée en sensibilité et spécificité et des modèles peuvent être choisis en fonction des seuils d'équilibre de ces valeurs.\n",
    "\n",
    "Tout cela sera vu au prochain bloc sur l'évaluation de modèles.\n",
    "\n",
    "#### Ré-échantillonner le jeu de données\n",
    "\n",
    "Il est possible de modifier le jeu de données utilisé afin d'obtenir des données plus équilibrées et améliorer le modèle prédictif.\n",
    "\n",
    "Cette modification s'appelle [échantillonnage du dataset](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis). Il existe deux méthodes principales pour uniformiser les classes:\n",
    "- l'ajout de copies d'instances de la classe sous-représentée : sur-échantillonnage (ou plus formellement échantillonner avec remplacement), ou\n",
    "- la suppression d'instances de la classe surreprésentée : sous-échantillonnage.\n",
    "\n",
    "Ces approches sont souvent très faciles à mettre en œuvre et rapides à exécuter. Elles sont un excellent point de départ.\n",
    "\n",
    "En fait, il vaut mieux toujours essayer les deux approches sur tous les jeux de données déséquilibrés, juste pour voir si cela donne une amélioration de vos mesures préférées.\n",
    "\n",
    "Quelques règles de base :\n",
    "- Le sous-échantillonnage est préférable lorsque vous avez beaucoup de données (des dizaines, des centaines de milliers d'instances ou plus).\n",
    "- Le suréchantillonnage est préférable lorsque vous n’avez pas beaucoup de données (des dizaines de milliers d’enregistrements ou moins).\n",
    "- Tester des schémas d'échantillonnage aléatoire et non aléatoire (par exemple stratifié).\n",
    "- Tester différents ratios de rééchantillonnage (il n'est pas toujours nécessaire d'avoir un ratio de 1:1 dans un problème de classification binaire).\n",
    "\n",
    "#### Générer des échantillons synthétiques\n",
    "\n",
    "Un moyen simple de générer des échantillons synthétiques consiste à échantillonner de manière aléatoire les attributs d'instances de la classe minoritaire.\n",
    "\n",
    "Il est possible de les échantillonner de manière empirique dans votre jeu de données ou d'utiliser une méthode telle que Naive Bayes, qui permet d'échantillonner chaque attribut indépendamment lorsqu'il est exécuté à l'envers. Plus de données différentes seront générées, mais les relations non-linéaires entre les attributs peuvent ne pas être préservées.\n",
    "\n",
    "Il existe des algorithmes systématiques utilisables pour générer des échantillons synthétiques. Le plus populaire de ces algorithmes est appelé SMOTE ou technique de suréchantillonnage minoritaire synthétique.\n",
    "\n",
    "SMOTE est une méthode de suréchantillonnage. Cela fonctionne en créant des échantillons synthétiques de la classe mineure au lieu de créer des copies. L'algorithme sélectionne deux instances similaires ou plus (à l'aide d'une mesure de distance) et perturbe une instance à la fois d'une quantité aléatoire dans la différence des instances voisines.\n",
    "\n",
    "Il existe plusieurs implémentations de l'algorithme SMOTE. En Python, le module \"[`imbalanced-learn`](https://github.com/fmfn/UnbalancedDataset)\" fournit un certain nombre d'implémentations de SMOTE ainsi que diverses autres techniques de ré-échantillonnage.\n",
    "\n",
    "#### Essayer les modèles pénalisés\n",
    "\n",
    "Utilisation des mêmes algorithmes habituels en leur donnant une perspective différente du problème.\n",
    "\n",
    "La classification pénalisée impose un coût supplémentaire au modèle pour faire des erreurs de classification à la classe minoritaire pendant l'entrainement. Ces pénalités peuvent inciter le modèle à accorder plus d’attention à la classe minoritaire.\n",
    "\n",
    "Souvent, le traitement des pénalités de classe ou des poids est spécialisé dans l’algorithme d’apprentissage. Il existe des versions pénalisées d'algorithmes tels que les SVMs ou LDAs.\n",
    "\n",
    "Il est également possible d'avoir des cadres génériques pour les modèles pénalisés. Par exemple, Weka a un classificateur CostSensitive qui peut envelopper tout classificateur et appliquer une matrice de pénalités personnalisée pour la classification des échecs.\n",
    "\n",
    "L’utilisation de la pénalisation est souhaitable si vous êtes bloqué dans un algorithme spécifique et êtes incapable de rééchantillonner ou si vous obtenez des résultats médiocres. Il fournit un autre moyen d’équilibrer les cours. La mise en place de la matrice de pénalités peut être complexe. Vous devrez très probablement essayer divers systèmes de pénalités et voir ce qui convient le mieux à votre problème.\n",
    "\n",
    "#### Essayer une perspective différente\n",
    "\n",
    "Il existe des domaines d'étude dédiés aux jeux de données déséquilibrés. Ils ont leurs propres algorithmes, mesures et terminologie.\n",
    "\n",
    "Jeter un coup d'œil et réfléchir à votre problème sous ces angles peut parfois faire honte à certaines idées.\n",
    "\n",
    "La détection des anomalies et la détection des modifications sont deux éléments qui pourraient vous intéresser.\n",
    "\n",
    "La détection d'anomalie est la détection d'événements rares. Il s’agit peut-être d’un dysfonctionnement de la machine signalé par ses vibrations ou d’une activité malveillante d’un programme indiqué par sa séquence d’appels système. Les événements sont rares et comparés au fonctionnement normal.\n",
    "\n",
    "Ce changement de mentalité considère la classe mineure comme la classe des valeurs aberrantes, ce qui peut vous aider à réfléchir à de nouvelles façons de séparer et de classer les échantillons.\n",
    "\n",
    "La détection de changement est similaire à la détection d'anomalie, sauf que plutôt que de rechercher une anomalie, elle recherche un changement ou une différence. Il peut s’agir d’un changement de comportement d’un utilisateur, tel qu’observé par les modèles d’utilisation ou les transactions bancaires.\n",
    "\n",
    "Ces deux évolutions adoptent une approche plus en temps réel du problème de classification qui pourrait vous donner de nouvelles façons de penser à votre problème et peut-être d’autres techniques à essayer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de données débalancées\n",
    "\n",
    "* Jeu de données débalancé\n",
    "* Le piège métrique\n",
    "* Matrice de confusion\n",
    "* Rééchantillonnage\n",
    "* Sous-échantillonnage aléatoire\n",
    "* Suréchantillonnage aléatoire\n",
    "* Module d'apprentissage en Python déséquilibré\n",
    "* Sous-échantillonnage aléatoire et sur-échantillonnage avec apprentissage déséquilibré\n",
    "* Sous-échantillonnage: liens Tomek\n",
    "* Sous-échantillonnage: Centroids de cluster\n",
    "* Suréchantillonnage: SMOTE\n",
    "* Sur-échantillonnage suivi d'un sous-échantillonnage\n",
    "* Plus de liens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e38ddf38-d027-4eae-95c8-749f8f40db43",
    "_uuid": "1e287038acf96fc6d6825e77ba97b03f0824be0a"
   },
   "source": [
    "#### Données débalancées\n",
    "\n",
    "Dans cette partie, nous passons en revue certaines techniques permettant de gérer des ensembles de données très débalancées, en mettant l’accent sur le ré-échantillonnage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3b84706a-1eb0-435f-b1ff-3f1732cc3ae4",
    "_uuid": "63eb4f34b9a7d5106da4fe6c1c871bb0025c1ae4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "major = 0.99\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_classes=2, class_sep=1.5, weights=[major, 1-major],\n",
    "    n_informative=3, n_redundant=1, flip_y=0,\n",
    "    n_features=20, n_clusters_per_class=1,\n",
    "    n_samples=100000, random_state=10\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df['target'] = y\n",
    "target_count = df.target.value_counts()\n",
    "print('Class 0:', target_count[0])\n",
    "print('Class 1:', target_count[1])\n",
    "print('Proportion:', round(target_count[0] / target_count[1], 4), ': 1')\n",
    "df.target.value_counts().plot(kind='bar', title='Count (target)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c91f9c5d-05d2-478e-9dfb-1cb848bc0fe4",
    "_uuid": "8683656d1bfdef6b5c0c623597dcbc4160a0edc1"
   },
   "source": [
    "#### Le piège des métriques\n",
    "\n",
    "L'un des principaux problèmes auxquels se heurtent les utilisateurs novices lorsqu'ils traitent des ensembles de données non équilibrés est lié aux métriques utilisées pour évaluer leur modèle. Utiliser des métriques plus simples comme l'exactitude (`accuracy`) peut être trompeur. Dans un ensemble de données avec des classes très déséquilibrées, si le classificateur \"prédit\" toujours la classe la plus courante sans effectuer d'analyse des caractéristiques, il conservera un taux de précision élevé, évidemment illusoire.\n",
    "\n",
    "Faisons cette expérience en utilisant une simple validation croisée et aucune ingénierie de caractéristiques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0efbfb8a-76ab-4a86-b093-25271dcfcc06",
    "_uuid": "6d33cfa94ebb019b7c3065ea5f5dbe99ae52aeb1"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "model = XGBClassifier()\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.4f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c46b98a7-d500-4911-a7b5-c8fc8fbed069",
    "_uuid": "5b17ee8d3629cc346398e63269205e5b654cef80"
   },
   "source": [
    "Exécutons maintenant le même code, mais en utilisant une seule caractéristique (ce qui devrait considérablement réduire l'exactitude du classificateur):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bd97a34a-8e27-4eb2-a552-fe21f860dd15",
    "_uuid": "5c7c4e3f364c664f52d04d24f948cc416d2d7e4c"
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train[:,0:2], y_train)\n",
    "y_pred = model.predict(X_test[:,0:2])\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: %.4f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "68d4ffaf-4412-4ff3-a9cd-38bea195da3b",
    "_uuid": "d9d8b57f458bdd107ae08b76b0008d80e0674d97"
   },
   "source": [
    "Comme nous pouvons le constater, le taux de précision élevé n’était qu’une illusion. De cette manière, le choix de la métrique utilisée dans les jeux de données non équilibrés est extrêmement important. Dans cette compétition, la métrique d'évaluation était le [Coefficient de Gini normalisé](https://en.wikipedia.org/wiki/Gini_coefficient), une métrique plus robuste pour les jeux de données déséquilibrés, qui varie d'environ 0 pour une estimation aléatoire à environ 0,5 pour un score parfait."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e53e9f0f-8888-4fd9-b0d8-3bc937448162",
    "_uuid": "a176bd80315afd0f7c21fd26ab1841867a5eb7d0"
   },
   "source": [
    "#### Matrice de Confusion\n",
    "\n",
    "Une méthode intéressante pour évaluer les résultats consiste à utiliser une matrice de confusion qui montre les prédictions correctes et incorrectes pour chaque classe. Dans la première ligne, la première colonne indique combien de classes 0 ont été prédites correctement, et dans la seconde colonne, combien de classes 0 ont été prédites à 1. Dans la deuxième ligne, nous notons que toutes les entrées de classe 1 ont été prédites à tort comme étant la classe 0.\n",
    "\n",
    "Par conséquent, plus les valeurs diagonales de la matrice de confusion sont élevées, mieux ce sera, ce qui indique de nombreuses prédictions correctes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "394daa2b-700c-45fc-99d7-db08245a7697",
    "_uuid": "39c3256de9817f64c9ae47de5f1f78531d373015"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print('Matrice de confusion:\\n', conf_mat)\n",
    "\n",
    "labels = ['Class 0', 'Class 1']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(conf_mat, cmap=plt.cm.jet_r)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Expected')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b27346eb-7bf3-4360-993a-fb91e62bb937",
    "_uuid": "875f5ab3b5afcdaf3c7754ce957cb01fd32bf65c"
   },
   "source": [
    "#### Rééchantillonage\n",
    "\n",
    "Le rééchantillonnage est une technique largement adoptée pour traiter les jeux de données très déséquilibrés. Cela consiste à retirer des échantillons de la classe majoritaire (sous-échantillonnage) et / ou à ajouter d'autres exemples de la classe minoritaire (sur-échantillonnage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "03d31a16-7b66-4096-88d7-d548db734390",
    "_uuid": "2232ac0fb192a468486b400846f88913a36957e6"
   },
   "source": [
    "![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/resampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0df1a3f3-49ff-4ada-80f1-198bbbd79525",
    "_uuid": "67e203e0919c818e871650ef194fe497df2d39b5"
   },
   "source": [
    "Malgré l'avantage de l'équilibrage des classes, ces techniques ont aussi leurs faiblesses (`No Free Lunch`). La mise en œuvre la plus simple du suréchantillonnage consiste à dupliquer aléatoirement des données  de la classe de minorité, ce qui peut entraîner une sur-apprentissage. Dans le sous-échantillonnage, la technique la plus simple consiste à supprimer aléatoirement des enregistrements de la classe majoritaire, ce qui peut entraîner une perte d'information.\n",
    "\n",
    "Implémentons un exemple de base, qui utilise la méthode `DataFrame.sample` pour obtenir aléatoirement des échantillons de chaque classe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2adbc1c9-8cdf-43d9-9a57-f24f503ba523",
    "_uuid": "f959c30be59ad1eccaed33f48a35d99be053b547"
   },
   "outputs": [],
   "source": [
    "# Nombre de valeurs par classe\n",
    "count_class_0, count_class_1 = df.target.value_counts()\n",
    "\n",
    "# Séparation des classes\n",
    "df_class_0 = df[df['target'] == 0]\n",
    "df_class_1 = df[df['target'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "152ea73a-aa23-4fd2-a3f5-1f55c69041ae",
    "_uuid": "b765be76a182930feb650b01dd4d1de90501bbce"
   },
   "source": [
    "#### Sous-échantillonnage aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a0e61bc8-cef1-4828-9c5f-49c1c50c522c",
    "_uuid": "2a667d73560fb6408897835809a9d67310b45323"
   },
   "outputs": [],
   "source": [
    "df_class_0_under = df_class_0.sample(count_class_1)\n",
    "df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "\n",
    "print('Sous-échantillonnage aléatoire:')\n",
    "print(df_test_under.target.value_counts())\n",
    "\n",
    "df_test_under.target.value_counts().plot(kind='bar', title='Count (target)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "be656d47-e529-4533-b975-cf6de072d959",
    "_uuid": "17192fe8557463941e3abd4633b58ced0037721b"
   },
   "source": [
    "#### Suréchantillonnage aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8ca1ac09-5d61-4cf7-99a4-e299f4955c97",
    "_uuid": "431942c08c59f0a6ae629c3c38485bf85d001892"
   },
   "outputs": [],
   "source": [
    "df_class_1_over = df_class_1.sample(count_class_0, replace=True)\n",
    "df_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "\n",
    "print('Suréchantillonnage aléatoire:')\n",
    "print(df_test_over.target.value_counts())\n",
    "\n",
    "df_test_over.target.value_counts().plot(kind='bar', title='Count (target)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9fd90ddc-f2fc-487d-b177-0c540daf2eff",
    "_uuid": "9672899d4029b71b72897927ce464d6d7427ce77"
   },
   "source": [
    "#### Module d'apprentissage en Python déséquilibré\n",
    "\n",
    "Un certain nombre de techniques de ré-échantillonnage plus sophistiquées ont été proposées dans la littérature scientifique.\n",
    "\n",
    "Par exemple, il est possible de regrouper les données de la classe majoritaire et d'effectuer le sous-échantillonnage en supprimant les données de chaque cluster, cherchant ainsi à préserver les informations. En sur-échantillonnage, au lieu de créer des copies exactes des données de la classe de minorité, nous pouvons introduire de petites variations dans ces copies, créant ainsi des échantillons synthétiques plus divers.\n",
    "\n",
    "Appliquons certaines de ces techniques de rééchantillonnage en utilisant la librairie Python [imbalanced-learn](http://contrib.scikit-learn.org/imbalanced-learn/stable/). Elle est compatible avec scikit-learn et fait partie des projets scikit-learn-contrib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fdf3f76d-aacb-4ccc-9649-736dce7a237c",
    "_uuid": "41b482ec89f13dd22e85612404300041a3e71deb"
   },
   "outputs": [],
   "source": [
    "#!pip install imblearn\n",
    "import imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5542beb4-6aee-401a-8bc0-2a522fbbe90b",
    "_uuid": "ff93d1707c416178c010c125319220b785dca984"
   },
   "source": [
    "Pour faciliter la visualisation, créons un petit échantillon de données non équilibré à l'aide de la méthode `make_classification`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e97abde1-c324-47b5-a33c-3b8b92268b5e",
    "_uuid": "3126327b6f4b4c469701ac7a76590f5e55e17076"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n",
    "    n_informative=3, n_redundant=1, flip_y=0,\n",
    "    n_features=20, n_clusters_per_class=1,\n",
    "    n_samples=100, random_state=10\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(X)\n",
    "df['target'] = y\n",
    "df.target.value_counts().plot(kind='bar', title='Count (target)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4de57657-3aed-4444-a609-318063391763",
    "_uuid": "d348b114ec1594eeefa0a67aab8b54e5b9ee2bdf"
   },
   "source": [
    "Nous allons également créer une fonction de tracé à 2 dimensions, <code> plot_2d_space </ code>, pour afficher la répartition des données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8003c84c-fee4-45c7-b490-90a185799760",
    "_uuid": "b2a02a4ff687fb099232a0468917ec8ab737d22d"
   },
   "outputs": [],
   "source": [
    "def plot_2d_space(X, y, label='Classes'):   \n",
    "    colors = ['#1F77B4', '#FF7F0E']\n",
    "    markers = ['o', 's']\n",
    "    for l, c, m in zip(np.unique(y), colors, markers):\n",
    "        plt.scatter(\n",
    "            X[y==l, 0],\n",
    "            X[y==l, 1],\n",
    "            c=c, label=l, marker=m\n",
    "        )\n",
    "    plt.title(label)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2c565bbc-39ed-45a2-8b3d-bc501e2510aa",
    "_uuid": "aabc110dd8d1b36345df6aada1c59b864e48e8e6"
   },
   "source": [
    "Étant donné que le jeu de données comporte de nombreuses dimensions (caractéristiques) et que nos graphiques seront en 2D, la taille du jeu de données est réduite par analyse en composantes principales (PCA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "22b689c8-b296-4736-9aa6-46f36283f91f",
    "_uuid": "f64eff19d1304d4bd1e92d2c51a8a953cd17d7f9"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "plot_2d_space(X, y, 'Imbalanced dataset (2 PCA components)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c3c9a24f-3cd0-4c8d-8a4d-4403c4f1a641",
    "_uuid": "0d7316b04837aa103003d667f63ecb05d43fc04e"
   },
   "source": [
    "#### Sous-échantillonnage aléatoire et sur-échantillonnage avec apprentissage déséquilibré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "28bdbe23-5eeb-4335-8c70-7a00bbac3e03",
    "_uuid": "8a56a4b118d7cae885e4c6a45fa02b2f066ece78"
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(return_indices=True)\n",
    "X_rus, y_rus, id_rus = rus.fit_sample(X, y)\n",
    "\n",
    "print('Indices retirés :', id_rus)\n",
    "\n",
    "plot_2d_space(X_rus, y_rus, 'Sous-échantillonnage aléatoire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c8eaed30-339e-4d23-a0f2-fbd687b217ea",
    "_uuid": "be5b90300f0a25bbe7d1822503e7fc5185a906b2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "X_ros, y_ros = ros.fit_sample(X, y)\n",
    "\n",
    "print(X_ros.shape[0] - X.shape[0], 'nouveaux points aléatoires')\n",
    "\n",
    "plot_2d_space(X_ros, y_ros, 'Sur-échantillonnage aléatoire')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3156134f-539b-48b8-b9d0-64095fe50c1c",
    "_uuid": "b3f9ac47a157d9096a626360408859b795299c24"
   },
   "source": [
    "#### Sous-échantillonnage: liens Tomek\n",
    "\n",
    "Les liens Tomek sont des paires d'instances très proches, mais de classes opposées. La suppression des occurrences de la classe majoritaire de chaque paire augmente l'espace entre les deux classes, facilitant ainsi le processus de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f131f7e0-007d-406e-9e1c-db352d2d8433",
    "_uuid": "85c01c0e6baa1b984585c1db34c5ab0315cbf8ff"
   },
   "source": [
    "![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/tomek.png?v=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "45bf057b-369c-4f37-a5ac-7417ad79253d",
    "_uuid": "733a86ccfaaabf701fb2d1f9997c732b19630df3"
   },
   "source": [
    "Dans le code ci-dessous, nous utiliserons `ratio = 'majority'` pour rééchantillonner la classe majoritaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "58f13562-c0c6-4247-bf40-c6dec764ff3a",
    "_uuid": "0c93b1e11359f4d1eb873280b61aa0f54ee8bd36"
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "tl = TomekLinks(return_indices=True, ratio='majority')\n",
    "X_tl, y_tl, id_tl = tl.fit_sample(X, y)\n",
    "\n",
    "print('Indices retirés:', id_tl)\n",
    "\n",
    "plot_2d_space(X_tl, y_tl, ' Sous-échantillonnage des liens Tomek')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fef831bd-ecec-429c-aef2-5d51d1188820",
    "_uuid": "b4e75fffe4c91afcd63705aa7bcb16b6fd9f6b1f"
   },
   "source": [
    "#### Sous-échantillonnage: Centroids de cluster\n",
    "\n",
    "Cette technique effectue un sous-échantillonnage en générant des centroïdes basés sur des méthodes de clustering. Les données seront préalablement regroupées par similarité, afin de préserver les informations.\n",
    "\n",
    "Dans cet exemple, le dictionnaire `{0: 10}` pour le paramètre `ratio`, indique de préserver 10 éléments de la classe majoritaire (0) et conserver toutes les données de la classe minoritaire (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "91001b3e-fe5c-4dc1-8501-395e1218cba1",
    "_uuid": "ad088a08f4f9e0b646571950928c7fff47f52f98"
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "cc = ClusterCentroids(ratio={0: 10})\n",
    "X_cc, y_cc = cc.fit_sample(X, y)\n",
    "\n",
    "plot_2d_space(X_cc, y_cc, 'Cluster Centroids under-sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "78adb7b4-a7e1-4d10-9cc2-6bf6477c63df",
    "_uuid": "b3741f5c14acdbd76e25725e2d73df5f2cb0a239"
   },
   "source": [
    "#### Suréchantillonnage: SMOTE\n",
    "\n",
    "SMOTE (Synthetic Minority Oversampling TEchnique) consiste à synthétiser des éléments pour la classe minoritaire, sur la base de ceux qui existent déjà. Cela fonctionne de manière aléatoire en sélectionnant un point de la classe minoritaire et en calculant les k-voisins les plus proches pour ce point (k-NN). Les points synthétiques sont ajoutés entre le point choisi et ses voisins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5162646f-da82-4877-b6d8-25a8be7f42e9",
    "_uuid": "51697e21b7cdb4064dda18aa24e6ecf039b1132b"
   },
   "source": [
    " ![](https://raw.githubusercontent.com/rafjaa/machine_learning_fecib/master/src/static/img/smote.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9c0a0d78-8427-437e-aa24-ffe2bd12edb2",
    "_uuid": "9393851db694c178faf93615bf05addedf5d678b"
   },
   "source": [
    "Nous allons utiliser `ratio = 'minority'` pour rééchantillonner la classe de minorité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "97e9f84e-0951-4037-b882-57545f9d967a",
    "_uuid": "74457c951aabf5b16be1c4282c15d9cb2034f26b"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(ratio='minority')\n",
    "X_sm, y_sm = smote.fit_sample(X, y)\n",
    "\n",
    "plot_2d_space(X_sm, y_sm, 'SMOTE over-sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "20c3cdbb-a7c1-45a5-8dd9-84cff7d9af31",
    "_uuid": "d7ebbeb741ad6469cb7ebbced3499acd3c49856a"
   },
   "source": [
    "#### Sur-échantillonnage suivi d'un sous-échantillonnage\n",
    "\n",
    "Nous pouvons maintenant combiner le suréchantillonnage et le suréchantillonnage, en utilisant les techniques des liens SMOTE et Tomek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ab732fe1-45c1-4163-b70e-7a2ce2dbe42f",
    "_uuid": "b740fbaf8677522d3b3040e2f47e3b954dc56877"
   },
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "smt = SMOTETomek(ratio='auto')\n",
    "X_smt, y_smt = smt.fit_sample(X, y)\n",
    "\n",
    "plot_2d_space(X_smt, y_smt, 'SMOTE + Tomek links')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "28d4431f-bc32-4cf0-988e-9df412cc22c1",
    "_uuid": "401bcb8e508e584feca3a34ecfb9de270fde951c"
   },
   "source": [
    "#### Plus de liens\n",
    "\n",
    "* Documentation imbalanced-learn :<br>\n",
    "http://contrib.scikit-learn.org/imbalanced-learn/stable/index.html\n",
    "* GitHub imbalanced-learn :<br>\n",
    "https://github.com/scikit-learn-contrib/imbalanced-learn\n",
    "* Comparaison de la combinaison des algorithmes de sur- et sous-échantillonnage:<br>\n",
    "http://contrib.scikit-learn.org/imbalanced-learn/stable/auto_examples/combine/plot_comparison_combine.html\n",
    "* Chawla, Nitesh V., et al. \"SMOTE: synthetic minority over-sampling technique.\" Journal of artificial intelligence research 16 (2002):<br>\n",
    "https://www.jair.org/media/953/live-953-2037-jair.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation des données\n",
    "\n",
    "### Rééchelonnage ou normalisation\n",
    "\n",
    "Séquence inspirée des exemples de code de la librairie `scikit-learn` de [Raghav RV](mailto:rvraghav93@gmail.com), [Guillaume Lemaitre](mailto:g.lemaitre58@gmail.com) et Thomas Unterthiner (License: BSD 3 clause)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "cmap = getattr(cm, 'jet_r', cm.jet_r)\n",
    "\n",
    "def create_axes(title, figsize=(16, 6)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    # define the axis for the first plot\n",
    "    left, width = 0.1, 0.22\n",
    "    bottom, height = 0.1, 0.7\n",
    "    bottom_h = height + 0.15\n",
    "    left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.1]\n",
    "    rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    ax_scatter = plt.axes(rect_scatter)\n",
    "    ax_histx = plt.axes(rect_histx)\n",
    "    ax_histy = plt.axes(rect_histy)\n",
    "\n",
    "    # define the axis for the zoomed-in plot\n",
    "    left = width + left + 0.2\n",
    "    left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.1]\n",
    "    rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    ax_scatter_zoom = plt.axes(rect_scatter)\n",
    "    ax_histx_zoom = plt.axes(rect_histx)\n",
    "    ax_histy_zoom = plt.axes(rect_histy)\n",
    "\n",
    "    # define the axis for the colorbar\n",
    "    left, width = width + left + 0.13, 0.01\n",
    "\n",
    "    rect_colorbar = [left, bottom, width, height]\n",
    "    ax_colorbar = plt.axes(rect_colorbar)\n",
    "\n",
    "    return ((ax_scatter, ax_histy, ax_histx),\n",
    "            (ax_scatter_zoom, ax_histy_zoom, ax_histx_zoom),\n",
    "            ax_colorbar)\n",
    "\n",
    "\n",
    "def plot_distribution(axes, X, y, hist_nbins=50, title=\"\", x0_label=\"\", x1_label=\"\"):\n",
    "    ax, hist_X1, hist_X0 = axes\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x0_label)\n",
    "    ax.set_ylabel(x1_label)\n",
    "\n",
    "    # The scatter plot\n",
    "    colors = cmap(y)\n",
    "    ax.scatter(X[:, 0], X[:, 1], alpha=0.5, marker='o', s=5, lw=0, c=colors)\n",
    "\n",
    "    # Histogramme pour axe X1 (feature 5)\n",
    "    hist_X1.set_ylim(ax.get_ylim())\n",
    "    hist_X1.hist(X[:, 1], bins=hist_nbins, orientation='horizontal', color='grey', ec='grey')\n",
    "    hist_X1.axis('off')\n",
    "\n",
    "    # Histogramme pour axe X0 (feature 0)\n",
    "    hist_X0.set_xlim(ax.get_xlim())\n",
    "    hist_X0.hist(X[:, 0], bins=hist_nbins, orientation='vertical', color='grey', ec='grey')\n",
    "    hist_X0.axis('off')\n",
    "\n",
    "def make_plot(title, X):\n",
    "    ax_zoom_out, ax_zoom_in, ax_colorbar = create_axes(title)\n",
    "    axarr = (ax_zoom_out, ax_zoom_in)\n",
    "    plot_distribution(axarr[0], X, y, hist_nbins=200,\n",
    "                      x0_label=\"Revenu médian\",\n",
    "                      x1_label=\"Nombre d'occupants\",\n",
    "                      title=\"Données complètes\")\n",
    "\n",
    "    # On enleve les outliers \n",
    "    zoom_in_percentile_range = (0, 99)\n",
    "    cutoffs_X0 = np.percentile(X[:, 0], zoom_in_percentile_range)\n",
    "    cutoffs_X1 = np.percentile(X[:, 1], zoom_in_percentile_range)\n",
    "\n",
    "    non_outliers_mask = (\n",
    "        np.all(X > [cutoffs_X0[0], cutoffs_X1[0]], axis=1) &\n",
    "        np.all(X < [cutoffs_X0[1], cutoffs_X1[1]], axis=1))\n",
    "    plot_distribution(axarr[1], X[non_outliers_mask], y[non_outliers_mask],\n",
    "                      hist_nbins=200,\n",
    "                      x0_label=\"Revenu médian\",\n",
    "                      x1_label=\"Nombre d'occupants\",\n",
    "                      title=\"Sans Outliers\")\n",
    "\n",
    "    norm = mpl.colors.Normalize(y_full.min(), y_full.max())\n",
    "    mpl.colorbar.ColorbarBase(ax_colorbar, cmap=cmap,\n",
    "                              norm=norm, orientation='vertical',\n",
    "                              label='Valeur de la maison *100 K$')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Données brutes\n",
    "Doing nothing this print like that : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "# Données des maisons californiennes : La valeur à prédire est la valeur de la maison en centaine de milliers de dollars US\n",
    "dataset = fetch_california_housing()\n",
    "X_full, y_full = dataset.data, dataset.target\n",
    "#print(dataset.DESCR)\n",
    "# On ne prends que 2 caractéristiques pour faciliter la visualisation : la 0 est distribution 'heavy-tailed', et la 5 a quelques données aberrantes très prononcées\n",
    "X = X_full[:, [0, 5]]\n",
    "\n",
    "# # On reéchelonne la donnée de sortie entre 0 et 1 pour mieux visualiser la légende (étaler la couleur)\n",
    "y = minmax_scale(y_full)\n",
    "\n",
    "data = ('Unscaled data', X)\n",
    "make_plot(*data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardisation \n",
    "\n",
    "Pour rappel : $x_i = \\frac{x_i - \\bar{x}}{\\sigma(x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "data = ('Data after standard scaling', StandardScaler().fit_transform(X))\n",
    "make_plot(*data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Min-Max\n",
    "\n",
    "Pour rappel : $x_i = \\frac{x_i - \\min_i{x_i}}{\\max_i{x_i}-\\min_i{x_i}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = ('Data after min-max scaling', MinMaxScaler().fit_transform(X))\n",
    "make_plot(*data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max-Abs\n",
    "\n",
    "Pour rappel : $x_i = \\frac{x_i}{\\max_i{|x_i|}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "data = ('Data after max-abs scaling', MaxAbsScaler().fit_transform(X))\n",
    "make_plot(*data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Robuste (basé sur les quantiles 1 et 3)\n",
    "\n",
    "Pour rappel : $x_i = \\frac{x_i - Q_1(x)}{Q_3(x) - Q_1(x)}$\n",
    "\n",
    "Le centrage et la mise à l'échelle s'effectuent indépendamment sur chaque caractéristique en calculant les statistiques pertinentes sur les échantillons de l'ensemble d'apprentissage. La médiane et l'intervalle interquartile sont ensuite stockées pour être utilisées sur des données ultérieures à l'aide de la méthode de transformation.\n",
    "\n",
    "La standardisation est une pratique courante pour de nombreux estimateurs d'apprentissage automatique. Cela se fait généralement en supprimant la moyenne et en divisant par la variance. Cependant, les valeurs aberrantes peuvent souvent influencer négativement la moyenne/variance de l'échantillon. Dans ce cas, la médiane et l’intervalle interquartile donnent souvent de meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "data = ('Data after robust scaling', RobustScaler(quantile_range=(25, 75)).fit_transform(X))\n",
    "make_plot(*data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation par puissance\n",
    "\n",
    "Famille de transformations paramétriques et monotones appliquées pour rendre les données plus semblables à celles de Gauss. Ceci est utile pour modéliser les problèmes liés à l'hétéroscédasticité (variance non constante) ou à d'autres situations dans lesquelles une normalité est souhaitée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "data = ('Données après transformation par puissance (Yeo-Johnson)', PowerTransformer(method='yeo-johnson').fit_transform(X))\n",
    "make_plot(*data)\n",
    "data = ('Données après transformation par puissance (Box-Cox)', PowerTransformer(method='box-cox').fit_transform(X))\n",
    "make_plot(*data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation par Quantile\n",
    "\n",
    "La transformation est appliquée à chaque entité indépendamment. La fonction de densité cumulée d'une entité est utilisée pour projeter les valeurs d'origine. Les valeurs de caractéristiques des données nouvelles / invisibles qui sont inférieures ou supérieures à la plage ajustée seront mappées aux limites de la distribution en sortie. Notez que cette transformation est non linéaire. Cela peut fausser les corrélations linéaires entre les variables mesurées à la même échelle, mais rend les variables mesurées à différentes échelles plus directement comparables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "data = ('Données après transformation par quantile (gaussian)', QuantileTransformer(output_distribution='normal').fit_transform(X))\n",
    "make_plot(*data)\n",
    "data = ('Données après transformation par quantile (uniforme)', QuantileTransformer(output_distribution='uniform').fit_transform(X))\n",
    "make_plot(*data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisation \n",
    " \n",
    "Pour rappel : $x_i = \\frac{x_i}{ ||x_i|| }$\n",
    "\n",
    "La mise à l'échelle des entrées en fonction des normes de l'unité est une opération courante pour la classification de texte ou la mise en cluster, par exemple. Par exemple, le produit scalaire de deux vecteurs TF-IDF normalisés en $L_2$ est la similarité cosinus des vecteurs et constitue la métrique de similarité de base pour le modèle d'espace vectoriel couramment utilisé par la communauté de récupération d'informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "data = ('Données après transformation par normalisation L2', Normalizer().fit_transform(X))\n",
    "make_plot(*data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Une autre vision de ces rééchantillonneurs\n",
    "\n",
    "Pour mieux comprendre la différence entre eux voici une visualisation de la caractéristique `Revenu médian` avant et après transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfX = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "dfX = dfX.drop(dataset.feature_names[1:],axis=1)\n",
    "\n",
    "#col = dfX.MedInc.values.reshape(-1, 1)\n",
    "\n",
    "scalers = [\n",
    "    ('Standard', StandardScaler()),\n",
    "    ('Min-Max', MinMaxScaler()),\n",
    "    ('Max-Abs', MaxAbsScaler()),\n",
    "    ('Robuste', RobustScaler(quantile_range=(25, 75))),\n",
    "    ('Quantile (gaussian)', QuantileTransformer(output_distribution='normal')),\n",
    "    ('Quantile (uniforme)', QuantileTransformer(output_distribution='uniform'))\n",
    "]\n",
    "\n",
    "for scaler in scalers:\n",
    "    dfX[scaler[0]] = scaler[1].fit_transform(col)\n",
    "    \n",
    "\n",
    "dfX.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "cpt = 1\n",
    "f = plt.figure(figsize=(30,12))\n",
    "for scaler in scalers:\n",
    "    name = scaler[0]\n",
    "    ax = f.add_subplot(2,len(scalers),cpt)\n",
    "    \n",
    "    sns.distplot(dfX.MedInc,ax=ax)\n",
    "    ax.axvline(dfX.MedInc.mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "    sns.distplot(dfX[name],ax=ax)\n",
    "    ax.axvline(dfX[name].mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "    ax.set_title(name)\n",
    "\n",
    "    ax = f.add_subplot(2,len(scalers),cpt+len(scalers))\n",
    "    g = sns.regplot(x=\"MedInc\", y=name, data=dfX,ax=ax)\n",
    "    cpt+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduction de la dimensionnalité\n",
    "\n",
    "Séquence inspirée des exemples de code de la librairie `scikit-learn` de [Fabian Pedregosa](mailto:fabian.pedregosa@inria.fr), [Olivier Grisel](mailto:olivier.grisel@ensta.org), [Mathieu Blondel](mailto:mathieu@mblondel.org), et Gael Varoquaux (License: BSD 3 clause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import offsetbox\n",
    "from sklearn import (manifold, datasets, decomposition, ensemble,\n",
    "                     discriminant_analysis, random_projection)\n",
    "\n",
    "digits = datasets.load_digits(n_class=6)\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "n_samples, n_features = X.shape\n",
    "n_neighbors = 30\n",
    "n_components = 2\n",
    "\n",
    "# On définit une fonction pour adfficher la projection\n",
    "def plot_embedding(X, title=None):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.Set1((y[i]+1) / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemples de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Afficher quelques images de chiffres pour l'exemple\n",
    "n_img_per_row = 20\n",
    "img = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))\n",
    "for i in range(n_img_per_row):\n",
    "    ix = 10 * i + 1\n",
    "    for j in range(n_img_per_row):\n",
    "        iy = 10 * j + 1\n",
    "        img[ix:ix + 8, iy:iy + 8] = X[i * n_img_per_row + j].reshape((8, 8))\n",
    "\n",
    "plt.imshow(img, cmap=plt.cm.binary)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title('Une sélection des 5 premiers chiffres de MNIST')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Projection aléatoire 2D (basiquement mélanger) à partir d'une matrice alétoire\n",
    "rp = random_projection.SparseRandomProjection(n_components=n_components, random_state=42)\n",
    "X_projected = rp.fit_transform(X)\n",
    "plot_embedding(X_projected, \"Projection aléatoire des chiffres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse en composante principales (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Projection des chiffres par les 2 principaux composants sur la matrice de covariance\n",
    "X_pca = decomposition.PCA(n_components=n_components).fit_transform(X)\n",
    "plot_embedding(X_pca, \"Projection des chiffres par Composante Principale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA pour données sparses (Latent Sementic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Projection des chiffres par les 2 principaux composants LSA (latent semantic analysis) = PCA pour données sparse.\n",
    "X_pca = decomposition.TruncatedSVD(n_components=n_components).fit_transform(X)\n",
    "plot_embedding(X_pca, \"Projection des chiffres par Composante Principale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse par discriminants linéaires (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Projection des chiffres par les deux principaux discriminants linéaires\n",
    "X2 = X.copy()\n",
    "X2.flat[::X.shape[1] + 1] += 0.01  # X doit être inversable ... \n",
    "X_lda = discriminant_analysis.LinearDiscriminantAnalysis(n_components=n_components).fit_transform(X2, y)\n",
    "plot_embedding(X_lda, \"Projection des chiffres par Discrimination Linéaire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Projection des chiffres par Isomap\n",
    "X_iso = manifold.Isomap(n_neighbors, n_components=n_components).fit_transform(X)\n",
    "plot_embedding(X_iso, \"Projection des chiffres par Isomap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encodage Locallement Linéaire (LLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Encodage localement linéaire des chiffres\n",
    "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=n_components, method='standard')\n",
    "X_lle = clf.fit_transform(X)\n",
    "plot_embedding(X_lle, \"Encodage localement lonéaire des chiffres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alignement des tangentes locales (LSTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# LTSA : Alignement des tangentes locales des chiffres\n",
    "clf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=n_components, method='ltsa')\n",
    "X_ltsa = clf.fit_transform(X)\n",
    "plot_embedding(X_ltsa,\n",
    "               \"LTSA : Alignement des tangentes locales des chiffres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multidimensional Scaling (MDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Encodage MDS des chiffres\n",
    "clf = manifold.MDS(n_components=n_components, n_init=1, max_iter=100)\n",
    "X_mds = clf.fit_transform(X)\n",
    "plot_embedding(X_mds, \"Encodage MDS des chiffres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forêt d'Isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Encodage par arbres aléatoires des chiffres\n",
    "hasher = ensemble.RandomTreesEmbedding(n_estimators=200, random_state=0, max_depth=5)\n",
    "X_transformed = hasher.fit_transform(X)\n",
    "pca = decomposition.TruncatedSVD(n_components=n_components)\n",
    "X_reduced = pca.fit_transform(X_transformed)\n",
    "\n",
    "plot_embedding(X_reduced, \"Encodage par arbres aléatoires des chiffres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encodage Spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Encodage Spectral des chiffres\n",
    "embedder = manifold.SpectralEmbedding(n_components=n_components, random_state=0, eigen_solver=\"arpack\")\n",
    "X_se = embedder.fit_transform(X)\n",
    "\n",
    "plot_embedding(X_se,\"Encodage Spectral des chiffres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Encodage t-SNE des chiffres\n",
    "# Visualizing : https://github.com/oreillymedia/t-SNE-tutorial\n",
    "tsne = manifold.TSNE(n_components=n_components, init='pca', random_state=0)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plot_embedding(X_tsne,\"Encodage t-SNE des chiffres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables catégoriques\n",
    "\n",
    "**Regroupement de catégories en une seule valeur:**\n",
    "\n",
    "Exemple: Vous avez une variable catégorielle `pays` qui a 180 valeurs uniques. Vous voulez voir les 3 pays les plus importants seulement et placer tous les autres pays dans une valeur \"autre\".\n",
    "\n",
    "```\n",
    "# Afficher le nombre de valeurs uniques de chaque variable catégorique dans les données\n",
    "for i in df.columns:\n",
    "    if df[i].dtypes=='object': \n",
    "        unique_cat=len(df[i].unique())\n",
    "        print(\"La carctéristique '{i}' a {unique_cat} catégories uniques\".format(col_name=col_name, unique_cat=unique_cat))\n",
    "\n",
    "# Afficher le nombre de valeurs de chaque variable catégorique\n",
    "print(df['pays'].value_counts())\n",
    "\n",
    "# Catégoriser les catégories moins fréquentes comme \"autres\"\n",
    "def repl(x):\n",
    "    if x == 'US': return 'US'\n",
    "    elif x == 'BR': return 'BR'\n",
    "    elif x == 'ES': return 'ES'\n",
    "    else: return 'Other'\n",
    "    \n",
    "df['pays'] = df['COUNTRY'].apply(repl)\n",
    "print(df['pays'].value_counts().sort_values(ascending=False))\n",
    "```\n",
    "\n",
    "**Regroupement de variables numériques en catégories**\n",
    "\n",
    "**Catégorie de taille égales :**\n",
    "\n",
    "```\n",
    "df['var'] = 0\n",
    "var = pd.qcut(x=COL1, q=3, labels=[\"good\", \"medium\", \"bad\"])\n",
    "print(df['var'].value_counts().sort_values(ascending=False))\n",
    "```\n",
    "Ajustez le nombre de catégories (q) et les étiquettes.\n",
    "\n",
    "**Catégories par intervalles identiques :**\n",
    "\n",
    "```\n",
    "bins = [0, 20, 40, 60, 80, 100] \n",
    "df.var = pd.cut(x=COL1, bins, labels=['Very low', 'Low', 'Medium', 'High', 'Very high']) \n",
    "print(data['var'].value_counts().sort_values(ascending = False))\n",
    "```\n",
    "Ajustez les catégories en spécifiant les seuils (doit avoir un de plus que le nombre de catégories/étiquettes). Par défaut, les emplacements incluent le bord le plus à droite, définissez l'argument `right = False` si les limites de droite ne doivent pas être inclues.\n",
    "\n",
    "### Encoder des variables categoriques\n",
    "\n",
    "**Encoder une variable booléenne en la transformant en entier :**\n",
    "\n",
    "```\n",
    "df['BOOL'] = (df.COL1==\"ABC\").astype(int)\n",
    "dta.head()\n",
    "```\n",
    "Dans cet exemple, COL1 contient des valeurs de chaîne. BOOL sera égal à 1 si COL1 contient \"ABC\".\n",
    "\n",
    "**Encoder manuellement en mappant un dictionnaire :**\n",
    "\n",
    "```\n",
    "dic = {'Yes': 1, 'No': 2}\n",
    "df['VAR'] = df['VAR'].map(dic)\n",
    "```\n",
    "\n",
    "**Encoder automatiquement (OneHot) :**\n",
    "\n",
    "```\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "x = onehotencoder.fit_transform(x).toarray()\n",
    "\n",
    "```\n",
    "La colonne devant être encodée est spcécifiées dans le constructeur, [0] dans l'exemple. Les données `x` sont ensuite transformées avec l’objet onehotencoder. Après ces étapes il y a maintenant autant de nouvelles colonnes dans le jeu de données que de catégories dans la colonne [0].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oRTvQwg0Z-0l"
   },
   "source": [
    "# On essaie ?\n",
    "---\n",
    "\n",
    "On charge Pandas, Numpy et Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce dataset (trouvé sur [Kaggle](https://www.kaggle.com/lava18/google-play-store-apps) comme les autres), est un *scraping* d'environ 10000 applications qu'on retrouve sur le Google Play Store incluant les champs suivants : \n",
    "\n",
    "* App: Application name\n",
    "* Category: the app belongs to\n",
    "* Rating: Overall user rating of the app (as when scraped)\n",
    "* Reviews: Number of user reviews for the app (as when scraped)\n",
    "* Size: Size of the app (as when scraped)\n",
    "* Installs: Number of user downloads/installs for the app (as when scraped)\n",
    "* Type: Paid or Free\n",
    "* Price: Price of the app (as when scraped)\n",
    "* Content: Rating Age group the app is targeted at - Children / Mature 21+ / Adult\n",
    "* Genres: An app can belong to multiple genres (apart from its main category). For eg, a musical family game will belong to Music, Game, Family genres.\n",
    "* Last Updated: Date when the app was last updated on Play Store (as when scraped)\n",
    "* Current Ver: Current version of the app available on Play Store (as when scraped)\n",
    "* Android Ver: Min required Android version (as when scraped)\n",
    "\n",
    "On peut essayer de préparer adéquatement ce jeu de données en sachant que : \n",
    "* Les Ratings sont manquantes dans beaucoup de cas\n",
    "* Le Type et le prix sont débalancés (93/7) et donc peut-être difficile à prédire\n",
    "* Les données sont toutes (sauf les ratings) de type string donc si on veut régresser ou classifier il y'a du travail d'identification et de transformation à faire\n",
    "\n",
    "On essaie ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7RGf0WXRMv8S",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Kaggle dataset : https://www.kaggle.com/lava18/google-play-store-apps\n",
    "data = pd.read_csv('googleplaystore.csv')\n",
    "print(data.dtypes)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "462OA5bxS4tt"
   },
   "source": [
    "### Sources Web\n",
    "* http://scikit-learn.org\n",
    "* https://www.kaggle.com/timolee/a-home-for-pandas-and-sklearn-beginner-how-tos\n",
    "* https://elitedatascience.com/feature-engineering-best-practices\n",
    "* https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219\n",
    "* https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\n",
    "* https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\n",
    "\n",
    "\n",
    "<a id=\"note1\">Test MCAR de Little</a>\n",
    "\n",
    "Prendre la moyenne des données avec des valeurs manquantes et la moyenne des données sans valeurs manquantes. Si elles sont identiques/similaires, il est plus probable que vos données soient en MCAR (`Missing Completely At Random`).\n",
    "[1]\tRoderick J. A. Little. (1988). A Test of Missing Completely at Random for Multivariate Data with Missing Values. Journal of the American Statistical Association, 83(404), 1198-1202. doi:10.2307/2290157"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "DataPrep.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
